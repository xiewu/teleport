---
title: Managing Disaster Recovery in an Amazon EKS Teleport Cluster
description: Provides guidance for planning a strategy for restoring a self-hosted Teleport cluster on EKS after a regional outage.
---

In case of an outage in your cloud provider region, you need to ensure that you
can restore your Teleport cluster to working order. This guide provides an
overview of a disaster recovery approach for self-hosted Teleport clusters.

The guide assumes that your self-hosted Teleport cluster runs on Kubernetes
using Amazon Elastic Kubernetes Service and uses the `teleport-cluster` Helm
chart. The `teleport-cluster` Helm chart is the recommended approach for quickly
getting started with self-hosting a Teleport cluster, and you can read about how
to get started with the chart in the [Helm deployment
guides](../../helm-deployments//helm-deployments.mdx).

## How it works

In the approach we explain in this guide, the Teleport Auth Service backends
receive continuous backups to a secondary region. If the primary region becomes
unavailable due to an outage, an admin redeploys the cluster to the new region,
configuring the Teleport Auth Service to connect to the backends in the
secondary region. Since Teleport certificate authorities are already backed up
the new region, running Teleport Agents do not need to reconnect to the cluster,
and there is no need to recreate dynamic resources. Recovery time depends on the
time it takes to redeploy the Auth Service and Proxy Service in the new region.

## Architectural overview of the disaster recovery approach  

{/* TODO: Hugo feedback: "One very common mistake people are doing (and we keep
doing this at Teleport too ðŸ¥²) is think that outages are "clean", the resources
just disappear, VMs shutdown, ...

It's very rarely the case: zonal or regional failures usually affect only one
service or one aspect of the region, the rest of it is still running. E.g.
network becomes unstable, packet loss and latency increase. This is enough to
disrupt Teleport but VMs are still running and managing to do some operations.

Conversely, after the outage, the services coming back online can be as
disruptive as the services being down."*/}

{/*TODO: Hugo: "I suggest to strongly  communicate about the necessity stopping
the old cluster before attempting to deploy a new one. This might prove
difficult during an incident as things never break cleanly, but the first thing
to do is to turn the partial/shadow failure into a clean one by stopping any
process, and force-cutting already open network connections. Agents connections
are sticky and they will happily stay connected to the broken cluster, even days
after the DNS entry got changed." */}

### Load balancer  

{/* TODO: Hugo: "If you are using ACM, you might need to create the certificate
in ACM before. The same way you will need to create the S3 bucket, dynamodb
backend, and IAM roles prior running "helm install"." */}

### Cluster state backend (assuming DynamoDB)  

The Teleport cluster state backend stores certificate authorities and dynamic
resources {/*TODO: anything else?*/}. The most critical part of a disaster
recovery plan is to maintain a backup of the cluster state backend. 

With a backup available, the Teleport Auth Service can supply an existing
certificate authority to sign certificates for cluster components. If you
restore a Teleport cluster with a new backend, and do not have a backup, you
will need to configure cluster components, such as self-hosted databases
protected by Teleport, to trust the new CAs.

When executing the disaster recovery plan that we describe in this guide, the
Teleport Auth Service becomes available in a new AWS region and connects to a
cluster state backend with an existing backup. The main consideration for
estimating a recovery point objective is how long it will take for the Teleport
Auth Service to come online. Your Teleport cluster will lose state changes made
between the last backup and the time the new Auth Service instances become
available.

You can also make a backup of your cluster state outside of the continuous
DynamoDB backups by running the following commands. The following command
assumes that the Teleport Auth Service runs in the `teleport` namespace in your
EKS cluster:

```code
$ kubectl -n teleport exec -i deployment/teleport-cluster-auth -- tctl get all --with-secrets > state.yaml
```

{/*TODO: Hugo: "For KMS users, backing up their cluster might be harder than
"tctl get all" once in a while, or running an hourly DynamoDB to s3 export. They
must replicate the keys to the other region. See
https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html" */}

{/*Hugo: "AFAIK, Unlike what its name says, "tctl get all" doesn't get call
resources in the backend. I'm not familiar with dynamo but I would expect many
resources to go missing."*/}

### Route 53: Updating DNS records to point to the new cluster.  

Once you have launched your Teleport cluster in the new AWS region, you must
ensure that Route 53 DNS records point to the new cluster.

1. Ensure that existing DNS records for the Teleport Proxy Service in your
   cluster have a low TTL, e.g., one minute. We expect that DNS resolvers in
   your users' networks honor the TTLs of DNS records to prevent issues with
   propagating records.

(!docs/pages/includes/self-hosted-helm-dns.mdx!)

### Session recording backend (assuming S3)  

{/*TODO: Hugo: There's no global S3 bucket. You can create one by creating
transfer rules. This is described in the multi-region blueprint.
*/}

{/*TODO: Hugo: I think you might be confusing audit event and session records.
We need both S3 for recording, and Dynamo for events.*/}

### Auth Service/Proxy Service:   
{/* TODO Make changes to the Helm values file to apply to the new region (e.g., change the region) */}
{/* TODO Describe required IAM permissions for the Auth Service and Proxy Servicve  */}
{/* TODO Kubernetes operator procedure Hugo: "The good thing is that the
operator should not attempt to takeover Teleport resource created from the other
cluster if no CR exist. So I think it's OK to deploy with the operator. They
will need to re-create all the CRs in Kube, else they will have dangling
Teleport resources." */}
{/* TODO: Rerun "helm install" in the new region. */}

## Guidance  
### Recovery time objective  
{/*TODO: Expect 15-30m of recovery time to redeploy to a new region  

 Hugo: "This sounds very optimistic, I would not expect anyone to achieve this
 in less than an hour. Deploying the chart already takes several minutes to go
 from nothing to a ready Teleport cluster. If they are using cert-manager it can
 take up to 10 minutes."*/}

 {/* TODO: Shortening the RTO:  
      - Restoring tables in the new region prior to redeploying Teleport  
      - Having the Auth Service, Proxy Service, and DynamoDB already running in the new region*/}

### Imposing a change freeze during the disaster recovery window  

{/* TODO: Hugo: "This will not be enough to avoid some things to break. Bot
certs are renewed every 20 minutes. Bots whose cert have been renewed since the
last backup will automatically get locked out of the cluster because of the
certificate generation mismatch. If the outage takes more than 40 minutes, bot
certs will start to expire, also locking bots out of the cluster."

From the outline:
    - no CA rotations   
    - no dynamic resource creation/modification.  

*/}

### Test disaster recovery procedure to make sure it works.   

{/*TODO: Common causes of failure:   
      - misconfigured IAM settings  
      - incorrectly configured backend URLs in the Auth Service configuration*/}


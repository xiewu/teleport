---
title: Managing Disaster Recovery in an Amazon EKS Teleport Cluster
description: Provides guidance for planning a strategy for restoring a self-hosted Teleport cluster on EKS after a regional outage.
---

In case of an outage in your cloud provider region, you need to ensure that you
can restore your Teleport cluster to working order. This guide provides an
overview of a disaster recovery approach for self-hosted Teleport clusters.

The guide assumes that your self-hosted Teleport cluster runs on Kubernetes
using Amazon Elastic Kubernetes Service and uses the `teleport-cluster` Helm
chart. The `teleport-cluster` Helm chart is the recommended approach for quickly
getting started with self-hosting a Teleport cluster, and you can read about how
to get started with the chart in the [Helm deployment
guides](../../helm-deployments//helm-deployments.mdx).

## Architectural overview of the disaster recovery approach  

In the approach we explain in this guide, the Teleport Auth Service backends
receive continuous backups to a secondary region. If the primary region becomes
unavailable due to an outage, an admin redeploys the cluster to the new region,
configuring the Teleport Auth Service to connect to the backends in the
secondary region. Since Teleport certificate authorities are already backed up
the new region, running Teleport Agents do not need to reconnect to the cluster,
and there is no need to recreate dynamic resources. Recovery time depends on the
time it takes to redeploy the Auth Service and Proxy Service in the new region.

### Load balancer  

{/* TODO: Hugo: "If you are using ACM, you might need to create the certificate
in ACM before. The same way you will need to create the S3 bucket, dynamodb
backend, and IAM roles prior running "helm install"." */}

### Cluster state backend (assuming DynamoDB)  

The Teleport cluster state backend stores certificate authorities and dynamic
resources {/*TODO: anything else?*/}. The most critical part of a disaster
recovery plan is to maintain a backup of the cluster state backend. 

With a backup available, the Teleport Auth Service can supply an existing
certificate authority to sign certificates for cluster components. If you
restore a Teleport cluster with a new backend, and do not have a backup, you
will need to configure cluster components, such as self-hosted databases
protected by Teleport, to trust the new CAs.

When executing the disaster recovery plan that we describe in this guide, the
Teleport Auth Service becomes available in a new AWS region and connects to a
cluster state backend with an existing backup. The main consideration for
estimating a recovery point objective is how long it will take for the Teleport
Auth Service to come online. Your Teleport cluster will lose state changes made
between the last backup and the time the new Auth Service instances become
available.

You can also make a backup of your cluster state outside of the continuous
DynamoDB backups by running the following commands. The following command
assumes that the Teleport Auth Service runs in the `teleport` namespace in your
EKS cluster:

```code
$ kubectl -n teleport exec -i deployment/teleport-cluster-auth -- tctl get all --with-secrets > state.yaml
```

{/*TODO: Hugo: "For KMS users, backing up their cluster might be harder than
"tctl get all" once in a while, or running an hourly DynamoDB to s3 export. They
must replicate the keys to the other region. See
https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html" */}

{/*Hugo: "AFAIK, Unlike what its name says, "tctl get all" doesn't get call
resources in the backend. I'm not familiar with dynamo but I would expect many
resources to go missing."*/}

### Route 53: Updating DNS records to point to the new cluster.  

Once you have launched your Teleport cluster in the new AWS region, you must
ensure that Route 53 DNS records point to the new cluster.

1. Ensure that existing DNS records for the Teleport Proxy Service in your
   cluster have a low TTL, e.g., one minute. We expect that DNS resolvers in
   your users' networks honor the TTLs of DNS records to prevent issues with
   propagating records.

(!docs/pages/includes/self-hosted-helm-dns.mdx!)

### Session recording backend (assuming S3)  

{/*TODO: Hugo: There's no global S3 bucket. You can create one by creating
transfer rules. This is described in the multi-region blueprint.
*/}

{/*TODO: Hugo: I think you might be confusing audit event and session records.
We need both S3 for recording, and Dynamo for events.*/}

### Auth Service/Proxy Service:   

{/* TODO Make changes to the Helm values file to apply to the new region (e.g., change the region) */}
{/* TODO Describe required IAM permissions for the Auth Service and Proxy Servicve  */}
{/* TODO Kubernetes operator procedure Hugo: "The good thing is that the
operator should not attempt to takeover Teleport resource created from the other
cluster if no CR exist. So I think it's OK to deploy with the operator. They
will need to re-create all the CRs in Kube, else they will have dangling
Teleport resources."*/}

1. Install the `teleport-cluster` Helm chart using the new values:

   ```code
   $ helm install teleport-cluster teleport/teleport-cluster \
     --version (=teleport.version=) \
     --values teleport-cluster-values.yaml
   ```

1. After installing the `teleport-cluster` chart, wait a minute or so and ensure
   that both the Auth Service and Proxy Service pods are running:

   ```code
   $ kubectl get pods
   NAME                                      READY   STATUS    RESTARTS   AGE
   teleport-cluster-auth-000000000-00000     1/1     Running   0          114s
   teleport-cluster-proxy-0000000000-00000   1/1     Running   0          114s
   ```

## Guidance  

{/* TODO: intro paragraph*/}

### Stopping the existing cluster

When you detect a zonal or regional failure, it is important that you cleanly
stop the xisting Teleport cluster. Zonal or regional failures usually affect
only one service or compute resource.  For example, the network becomes unstable
as packet loss and latency increase.  While outages at the cloud provider level
disrupt the expected functioning of a Teleport cluster, some operations may
continue successfully. After the outage has concluded, services that had
remained functional, or that come back online, can also become disruptive.

1. Stop any Teleport Auth Service and Proxy Service processes in your cluster.
   {/*TODO: Add an example command*/}
   
   {/*TODO: "force-cutting already open network connections. Agents connections
   are sticky and they will happily stay connected to the broken cluster, even
   days after the DNS entry got changed." */}

### Recovery time objective  
{/*TODO: Expect 15-30m of recovery time to redeploy to a new region  

 Hugo: "This sounds very optimistic, I would not expect anyone to achieve this
 in less than an hour. Deploying the chart already takes several minutes to go
 from nothing to a ready Teleport cluster. If they are using cert-manager it can
 take up to 10 minutes."*/}

 {/* TODO: Shortening the RTO:  
      - Restoring tables in the new region prior to redeploying Teleport  
      - Having the Auth Service, Proxy Service, and DynamoDB already running in the new region*/}

### Imposing a change freeze during the disaster recovery window  

{/* TODO: Hugo: "This will not be enough to avoid some things to break. Bot
certs are renewed every 20 minutes. Bots whose cert have been renewed since the
last backup will automatically get locked out of the cluster because of the
certificate generation mismatch. If the outage takes more than 40 minutes, bot
certs will start to expire, also locking bots out of the cluster."

From the outline:
    - no CA rotations   
    - no dynamic resource creation/modification.  

*/}

### Test your disaster recovery procedure

We strongly recommended testing your disaster recovery plan. Schedule time to
stop your Teleport cluster in one region and redeploy in another one using your
backup of the Auth Service cluster state backend, then ensure that users can
continue to connect to Teleport-protected resources.

Common causes of disaster recovery failures include:
- **Misconfigured IAM settings:** The Teleport Auth Service in your first region
  can access its backend, for example, but in the second region, the Auth
  Service has a role with insufficient permissions.
- **Misconfigured backend connections:** The Teleport Auth Service is configured
  with an incorrect cluster state backend URL in the new region, meaning that
  when it starts up, it fails to retrieve its existing CAs and dynamic
  resources, and bootstraps instead with a self-signed certificate.

---
title: Managing Disaster Recovery in an Amazon EKS Teleport Cluster
description: Provides guidance for planning a strategy for restoring a self-hosted Teleport cluster on EKS after a regional outage.
---

In case of an outage in your cloud provider region, you need to ensure that you
can restore your Teleport cluster to working order. This guide provides an
overview of a disaster recovery approach for self-hosted Teleport clusters.

The guide assumes that your self-hosted Teleport cluster runs on Kubernetes
using Amazon Elastic Kubernetes Service and uses the `teleport-cluster` Helm
chart. The `teleport-cluster` Helm chart is the recommended approach for quickly
getting started with self-hosting a Teleport cluster, and you can read about how
to get started with the chart in the [Helm deployment
guides](../../helm-deployments//helm-deployments.mdx).

## Architectural overview of the disaster recovery approach  

In the approach we explain in this guide, the Teleport Auth Service backends
receive continuous backups to a secondary region. If the primary region becomes
unavailable due to an outage, an admin redeploys the cluster to the new region,
configuring the Teleport Auth Service to connect to the backends in the
secondary region. Since Teleport certificate authorities are already backed up
the new region, running Teleport Agents do not need to reconnect to the cluster,
and there is no need to recreate dynamic resources. Recovery time depends on the
time it takes to redeploy the Auth Service and Proxy Service in the new region.

### Load balancer  

The `teleport-cluster` Helm chart deploys a Kubernetes service that, by default,
has the `LoadBalancer` type. Reinstalling the Helm chart in your new AWS region
recreates the load balancer. If you are using AWS Certificate Manager or
`cert-manager` to provision TLS credentials for your load balancer, you must
create a new certificate and private key before installing the chart in the new
region.

For guidance on configuring your EKS cluster to use ACM and `cert-manager` with
the Teleport Proxy Service load balancer, see [Configure TLS certificates for
Teleport](./aws/#step-47-configure-tls-certificates-for-teleport).

### Cluster state backend (assuming DynamoDB)  

The Teleport cluster state backend stores critical data such as certificate
authorities and dynamic resources. 

With a backup available, the Teleport Auth Service can supply an existing
certificate authority to sign certificates for cluster components. If you
restore a Teleport cluster with a new backend, and do not have a backup, you
will need to configure cluster components, such as self-hosted databases
protected by Teleport, to trust the new CAs.

When executing the disaster recovery plan that we describe in this guide, the
Teleport Auth Service becomes available in a new AWS region and connects to a
cluster state backend with an existing backup. The main consideration for
estimating a recovery point objective is how long it will take for the Teleport
Auth Service to come online. Your Teleport cluster will lose state changes made
between the last backup and the time the new Auth Service instances become
available.

You can also make a backup of your cluster state outside of the continuous
DynamoDB backups by running the following commands. The following command
assumes that the Teleport Auth Service runs in the `teleport` namespace in your
EKS cluster:

```code
$ kubectl -n teleport exec -i deployment/teleport-cluster-auth -- tctl get all --with-secrets > state.yaml
```

Note that the `tctl get all` command retrieves a predefines list of resource
types from the Auth Service state backend, and does not necessarily retrieve all
records from the backend. To ensure that your state backend in the new region is
identical to the one in the current region, you should rely on continuous
backups. If you want to retrieve or restore Auth Service resource state at a
particular point in time, we recommend managing Teleport dynamic resources using
infrastructure as code tools. See [Infrastructure as
Code](../../infrastructure-as-code/infrastructure-as-code.mdx) for more
information.

If the Teleport Auth Service in your cluster uses Amazon Key Management Service
for certificate authority private keys, you must replicate your keys to the new
AWS region before reinstalling the `teleport-cluster` Helm chart. See the [AWS
documentation](https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html")
for information about using KMS keys in multiple regions.

{/* TODO: How would a user enable KMS in the teleport-cluster Helm chart? I
don't see `ca_key_params` in the Auth Service values, which you need to enable
KMS:
https://goteleport.com/docs/admin-guides/deploy-a-cluster/aws-kms/#step-23-configure-your-auth-service-to-use-kms-keys
-*/}

### Route 53: Updating DNS records to point to the new cluster.  

Once you have launched your Teleport cluster in the new AWS region, you must
ensure that Route 53 DNS records point to the new cluster.

1. Ensure that existing DNS records for the Teleport Proxy Service in your
   cluster have a low TTL, e.g., one minute. We expect that DNS resolvers in
   your users' networks honor the TTLs of DNS records to prevent issues with
   propagating records.

(!docs/pages/includes/self-hosted-helm-dns.mdx!)

### Session recording backend

This guide assumes that you are using Amazon S3 for your session recording
backend. To have access to your session recordings in the case of region-wide
unavailability, you need to back up your session recordings to another region.
When you redeploy the Teleport Auth Service to the new region, you can configure
it to connet to an S3 bucket in the new region.

You can use S3 replication rules to create continuous backups from your
primary region to the backup region. See the [Multi-Region
Blueprint](../multi-region-blueprint.mdx#setting-up-multi-region-aws-s3-replication)
guide for information on setting up multi-region S3 bucket replication.

### Auth Service/Proxy Service

When deploying the Teleport Auth Service and Proxy Service to the new region,
you need to update the following fields in the Helm values file:

- `aws.region`
- `aws.backendTable`
- `aws.auditLogTable`
- `aws.sessionRecordingBucket`

You may need to update other values to match third-party dependencies of the
Teleport cluster. For example, if you plan to deploy `cert-manager` to handle
Teleport Proxy Service certificates instead of Let's Encrypt, you need to set
the `certManager.issuerName` field to match the name of the `cert-manager`
Issuer in your Kubernetes cluster. 

We recommend reading [Running an HA Teleport Cluster Using AWS, EKS, and
Helm](docs/pages/admin-guides/deploy-a-cluster/helm-deployments/aws.mdx) so you
can make sure that you have accounted for any third-party dependencies you need
to manage in the new AWS region.

{/* TODO Describe required IAM permissions for the Auth Service and Proxy Servicve  */}
{/* TODO Kubernetes operator procedure Hugo: "The good thing is that the
operator should not attempt to takeover Teleport resource created from the other
cluster if no CR exist. So I think it's OK to deploy with the operator. They
will need to re-create all the CRs in Kube, else they will have dangling
Teleport resources."*/}

1. Install the `teleport-cluster` Helm chart using the new values:

   ```code
   $ helm install teleport-cluster teleport/teleport-cluster \
     --version (=teleport.version=) \
     --values teleport-cluster-values.yaml
   ```

1. After installing the `teleport-cluster` chart, wait a minute or so and ensure
   that both the Auth Service and Proxy Service pods are running:

   ```code
   $ kubectl get pods
   NAME                                      READY   STATUS    RESTARTS   AGE
   teleport-cluster-auth-000000000-00000     1/1     Running   0          114s
   teleport-cluster-proxy-0000000000-00000   1/1     Running   0          114s
   ```

## Guidance  

Assuming that you have planned your disaster recovery strategy around the
architecture we lay out in the [previous
section](architectural-overview-of-the-disaster-recovery-approach), we recommend
the following practices.

### Stopping the existing cluster

When you detect a zonal or regional failure, it is important that you cleanly
stop the xisting Teleport cluster. Zonal or regional failures usually affect
only one service or compute resource.  For example, the network becomes unstable
as packet loss and latency increase.  While outages at the cloud provider level
disrupt the expected functioning of a Teleport cluster, some operations may
continue successfully. After the outage has concluded, services that had
remained functional, or that come back online, can also become disruptive.

1. Stop any Teleport Auth Service and Proxy Service processes in your cluster.
   {/*TODO: Add an example command*/}
   
   {/*TODO: "force-cutting already open network connections. Agents connections
   are sticky and they will happily stay connected to the broken cluster, even
   days after the DNS entry got changed." */}

### Testing your disaster recovery procedure

We strongly recommended testing your disaster recovery plan. Schedule time to
stop your Teleport cluster in one region and redeploy in another one using your
backup of the Auth Service cluster state backend, then ensure that users can
continue to connect to Teleport-protected resources.

Common causes of disaster recovery failures include:
- **Misconfigured IAM settings:** The Teleport Auth Service in your first region
  can access its backend, for example, but in the second region, the Auth
  Service has a role with insufficient permissions.
- **Misconfigured backend connections:** The Teleport Auth Service is configured
  with an incorrect cluster state backend URL in the new region, meaning that
  when it starts up, it fails to retrieve its existing CAs and dynamic
  resources, and bootstraps instead with a self-signed certificate.

### Shortening the recovery time objective

You can expect the disaster recovery procedure outlined in this guide to take at
least an hour, though the precise details depend on your infrastructure and
oragnization. To arrive at an exact benchmark, we strongly recommend [testing
your disaster recovery procedure](#testing-your-disaster-recovery-procedure).

You can take measures to shorten the recovery time objective of your disaster
recovery procedure. Possibilities include:

- Restore your cluster state and audit event backend tables from backup prior to
  reinstalling the `teleport-cluster` Helm chart.
- Have the Auth Service, Proxy Service, and backend services running in the new
  region. Read about the architecture of a multi-region Teleport deployment in
  the [Multi-Region Blueprint](../multi-region-blueprint.mdx) guide.

### Imposing a change freeze during the disaster recovery window  

{/* TODO: Hugo: "This will not be enough to avoid some things to break. Bot
certs are renewed every 20 minutes. Bots whose cert have been renewed since the
last backup will automatically get locked out of the cluster because of the
certificate generation mismatch. If the outage takes more than 40 minutes, bot
certs will start to expire, also locking bots out of the cluster."

From the outline:
    - no CA rotations   
    - no dynamic resource creation/modification.  

*/}



---
title: Managing Disaster Recovery in an Amazon EKS Teleport Cluster
description: Provides guidance for planning a strategy for restoring a self-hosted Teleport cluster on EKS after a regional outage.
---

In case of an outage in your cloud provider region, you need to ensure that you
can restore your Teleport cluster to working order. This guide provides an
overview of a disaster recovery approach for self-hosted Teleport clusters.

The guide assumes that your self-hosted Teleport cluster runs on Kubernetes
using Amazon Elastic Kubernetes Service and uses the `teleport-cluster` Helm
chart. The `teleport-cluster` Helm chart is the recommended approach for quickly
getting started with self-hosting a Teleport cluster, and you can read about how
to get started with the chart in the [Helm deployment
guides](../../helm-deployments//helm-deployments.mdx).

## How it works

In the approach we explain in this guide, the Teleport Auth Service backends
receive continuous backups to a secondary region. If the primary region becomes
unavailable due to an outage, an admin redeploys the cluster to the new region,
configuring the Teleport Auth Service to connect to the backends in the
secondary region. Since Teleport certificate authorities are already backed up
the new region, running Teleport Agents do not need to reconnect to the cluster,
and there is no need to recreate dynamic resources. Recovery time depends on the
time it takes to redeploy the Auth Service and Proxy Service in the new region.

## Architectural overview of the disaster recovery approach  

{/* TODO: Hugo feedback: "One very common mistake people are doing (and we keep
doing this at Teleport too ðŸ¥²) is think that outages are "clean", the resources
just disappear, VMs shutdown, ...

It's very rarely the case: zonal or regional failures usually affect only one
service or one aspect of the region, the rest of it is still running. E.g.
network becomes unstable, packet loss and latency increase. This is enough to
disrupt Teleport but VMs are still running and managing to do some operations.

Conversely, after the outage, the services coming back online can be as
disruptive as the services being down."*/}

{/*TODO: Hugo: "I suggest to strongly  communicate about the necessity stopping
the old cluster before attempting to deploy a new one. This might prove
difficult during an incident as things never break cleanly, but the first thing
to do is to turn the partial/shadow failure into a clean one by stopping any
process, and force-cutting already open network connections. Agents connections
are sticky and they will happily stay connected to the broken cluster, even days
after the DNS entry got changed." */}

### Load balancer  

{/* TODO: Hugo: "If you are using ACM, you might need to create the certificate
in ACM before. The same way you will need to create the S3 bucket, dynamodb
backend, and IAM roles prior running "helm install"." */}

### Cluster state backend (assuming DynamoDB)  

{/*TODO: Hugo: "For KMS users, backing up their cluster might be harder than
"tctl get all" once in a while, or running an hourly DynamoDB to s3 export. They
must replicate the keys to the other region. See
https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html"
*/}

{/* TODO: Explain the need to back up the cluster state backend to preserve the
Teleport CA for existing cluster components.  */}

{/* TODO: Recovery point objective implications: changes made to the Teleport
cluster during the time taken to complete the last backup would be lost on
restore.*/}

{/* TODO: Indicate that you can make a backup of cluster state outside DynamoDB
by running "tctl get all â€“with-secrets"

Hugo: "AFAIK, Unlike what its name says, "tctl get all" doesn't get call
resources in the backend. I'm not familiar with dynamo but I would expect many
resources to go missing."*/}

### Route 53: Updating DNS records to point to the new cluster.  

{/*TODO: Hugo: "For this procedure to work in most cases, users must set a low
TTL on Teleport's records. e.g. a minute.

This will not cover all cases, you will face DNS record propagation issues on
agents and clients running in corporate networks due to the terrible DNS
resolvers they use. There's nothing much we can do but tell people to use
resolvers honouring the record's TTLs." */}

### Session recording backend (assuming S3)  

{/*TODO: Hugo: There's no global S3 bucket. You can create one by creating
transfer rules. This is described in the multi-region blueprint.
*/}

{/*TODO: Hugo: I think you might be confusing audit event and session records.
We need both S3 for recording, and Dynamo for events.*/}

### Auth Service/Proxy Service:   
{/* TODO Make changes to the Helm values file to apply to the new region (e.g., change the region) */}
{/* TODO Describe required IAM permissions for the Auth Service and Proxy Servicve  */}
{/* TODO Kubernetes operator procedure Hugo: "The good thing is that the
operator should not attempt to takeover Teleport resource created from the other
cluster if no CR exist. So I think it's OK to deploy with the operator. They
will need to re-create all the CRs in Kube, else they will have dangling
Teleport resources." */}
{/* TODO: Rerun "helm install" in the new region. */}

## Guidance  
### Recovery time objective  
{/*TODO: Expect 15-30m of recovery time to redeploy to a new region  

 Hugo: "This sounds very optimistic, I would not expect anyone to achieve this
 in less than an hour. Deploying the chart already takes several minutes to go
 from nothing to a ready Teleport cluster. If they are using cert-manager it can
 take up to 10 minutes."*/}

 {/* TODO: Shortening the RTO:  
      - Restoring tables in the new region prior to redeploying Teleport  
      - Having the Auth Service, Proxy Service, and DynamoDB already running in the new region*/}

### Imposing a change freeze during the disaster recovery window  

{/* TODO: Hugo: "This will not be enough to avoid some things to break. Bot
certs are renewed every 20 minutes. Bots whose cert have been renewed since the
last backup will automatically get locked out of the cluster because of the
certificate generation mismatch. If the outage takes more than 40 minutes, bot
certs will start to expire, also locking bots out of the cluster."

From the outline:
    - no CA rotations   
    - no dynamic resource creation/modification.  

*/}

### Test disaster recovery procedure to make sure it works.   

{/*TODO: Common causes of failure:   
      - misconfigured IAM settings  
      - incorrectly configured backend URLs in the Auth Service configuration*/}

